#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£åˆ†å‰²å·¥å…·
ä¸“é—¨ç”¨äºå°†Markdownæ–‡æ¡£æŒ‰æ ‡é¢˜çº§åˆ«åˆ†å‰²ï¼Œä¸ºRAGç³»ç»Ÿåšå‡†å¤‡
"""

import json
from datetime import datetime
from typing import List, Dict, Optional


def read_and_split_document(file_path: str, split_level: str = "###") -> List[Dict]:
    """
    è¯»å–Markdownæ–‡æ¡£å¹¶æŒ‰æŒ‡å®šæ ‡é¢˜çº§åˆ«è¿›è¡Œåˆ†å‰²
    
    Args:
        file_path (str): æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        split_level (str): åˆ†å‰²çš„æ ‡é¢˜çº§åˆ«ï¼Œé»˜è®¤ä¸º"###"
        
    Returns:
        List[Dict]: åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - title: æ ‡é¢˜
            - content: å†…å®¹ï¼ˆåŒ…å«æ ‡é¢˜è¡Œï¼‰
            - level: æ ‡é¢˜çº§åˆ«
            - start_line: èµ·å§‹è¡Œå·
            - end_line: ç»“æŸè¡Œå·
            - word_count: å­—æ•°
            - char_count: å­—ç¬¦æ•°
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"âœ… æˆåŠŸè¯»å–æ–‡æ¡£: {file_path}")
        print(f"ğŸ“„ æ–‡æ¡£æ€»é•¿åº¦: {len(content):,} å­—ç¬¦")
        
        # æŒ‰è¡Œåˆ†å‰²å†…å®¹
        lines = content.split('\n')
        
        # å­˜å‚¨åˆ†å‰²åçš„æ–‡æ¡£å—
        document_chunks = []
        chunk_lines = []
        current_title = ""
        chunk_start_line = 0
        
        for i, line in enumerate(lines):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡å®šçº§åˆ«çš„æ ‡é¢˜
            if line.startswith(split_level + " "):
                # å¦‚æœä¹‹å‰æœ‰ç§¯ç´¯çš„å†…å®¹ï¼Œä¿å­˜ä¸ºä¸€ä¸ªå—
                if chunk_lines and current_title:
                    chunk_content = '\n'.join(chunk_lines).strip()
                    if chunk_content:  # åªä¿å­˜éç©ºå†…å®¹
                        document_chunks.append({
                            'title': current_title,
                            'content': chunk_content,
                            'level': len(split_level),
                            'start_line': chunk_start_line + 1,
                            'end_line': i,
                            'word_count': len(chunk_content),
                            'char_count': len(chunk_content.encode('utf-8'))
                        })
                
                # å¼€å§‹æ–°çš„å—
                current_title = line.replace(split_level + " ", "").strip()
                chunk_lines = [line]  # åŒ…å«æ ‡é¢˜è¡Œ
                chunk_start_line = i
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                chunk_lines.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if chunk_lines and current_title:
            chunk_content = '\n'.join(chunk_lines).strip()
            if chunk_content:
                document_chunks.append({
                    'title': current_title,
                    'content': chunk_content,
                    'level': len(split_level),
                    'start_line': chunk_start_line + 1,
                    'end_line': len(lines),
                    'word_count': len(chunk_content),
                    'char_count': len(chunk_content.encode('utf-8'))
                })
        
        print(f"âœ‚ï¸ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(document_chunks)} ä¸ªå—")
        return document_chunks
        
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []


def analyze_document_chunks(chunks: List[Dict]) -> Dict:
    """
    åˆ†ææ–‡æ¡£å—çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        chunks (List[Dict]): æ–‡æ¡£å—åˆ—è¡¨
        
    Returns:
        Dict: ç»Ÿè®¡ä¿¡æ¯
    """
    if not chunks:
        return {}
    
    total_chars = sum(chunk['char_count'] for chunk in chunks)
    total_words = sum(chunk['word_count'] for chunk in chunks)
    
    stats = {
        'total_chunks': len(chunks),
        'total_characters': total_chars,
        'total_words': total_words,
        'avg_chunk_size': total_chars // len(chunks) if chunks else 0,
        'min_chunk_size': min(chunk['char_count'] for chunk in chunks),
        'max_chunk_size': max(chunk['char_count'] for chunk in chunks),
        'chunks_info': []
    }
    
    for i, chunk in enumerate(chunks):
        stats['chunks_info'].append({
            'index': i,
            'title': chunk['title'],
            'char_count': chunk['char_count'],
            'lines': f"{chunk['start_line']}-{chunk['end_line']}"
        })
    
    return stats


def print_chunks_summary(chunks: List[Dict], show_details: bool = True) -> None:
    """
    æ‰“å°æ–‡æ¡£å—çš„æ‘˜è¦ä¿¡æ¯
    
    Args:
        chunks (List[Dict]): æ–‡æ¡£å—åˆ—è¡¨
        show_details (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    if not chunks:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£å—")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ–‡æ¡£åˆ†å‰²æ‘˜è¦")
    print(f"{'='*60}")
    
    stats = analyze_document_chunks(chunks)
    
    print(f"ğŸ“¦ æ€»å—æ•°: {stats['total_chunks']}")
    print(f"ğŸ“ æ€»å­—ç¬¦æ•°: {stats['total_characters']:,}")
    print(f"ğŸ“ å¹³å‡å—å¤§å°: {stats['avg_chunk_size']:,} å­—ç¬¦")
    print(f"ğŸ“ æœ€å°å—å¤§å°: {stats['min_chunk_size']:,} å­—ç¬¦")
    print(f"ğŸ“ æœ€å¤§å—å¤§å°: {stats['max_chunk_size']:,} å­—ç¬¦")
    
    if show_details:
        print(f"\nğŸ“‹ å„å—è¯¦ç»†ä¿¡æ¯:")
        print(f"{'åºå·':<4} {'æ ‡é¢˜':<35} {'å­—ç¬¦æ•°':<8} {'è¡Œå·èŒƒå›´':<12}")
        print("-" * 70)
        
        for info in stats['chunks_info']:
            title = info['title'][:32] + "..." if len(info['title']) > 35 else info['title']
            print(f"{info['index']:<4} {title:<35} {info['char_count']:<8} {info['lines']:<12}")


def save_chunks_to_json(chunks: List[Dict], output_file: str, metadata: Optional[Dict] = None) -> bool:
    """
    å°†æ–‡æ¡£å—ä¿å­˜ä¸ºJSONæ ¼å¼
    
    Args:
        chunks (List[Dict]): æ–‡æ¡£å—åˆ—è¡¨
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        metadata (Optional[Dict]): é¢å¤–çš„å…ƒæ•°æ®
        
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    try:
        # é»˜è®¤å…ƒæ•°æ®
        default_metadata = {
            'total_chunks': len(chunks),
            'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'split_method': 'markdown_heading_level_3'
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„å…ƒæ•°æ®
        if metadata:
            default_metadata.update(metadata)
        
        output_data = {
            'metadata': default_metadata,
            'chunks': chunks
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æ–‡æ¡£å—å·²ä¿å­˜åˆ°: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        return False


def get_chunk_by_title(chunks: List[Dict], title: str) -> Optional[Dict]:
    """
    æ ¹æ®æ ‡é¢˜è·å–æ–‡æ¡£å—
    
    Args:
        chunks (List[Dict]): æ–‡æ¡£å—åˆ—è¡¨
        title (str): è¦æŸ¥æ‰¾çš„æ ‡é¢˜
        
    Returns:
        Optional[Dict]: æ‰¾åˆ°çš„æ–‡æ¡£å—ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›None
    """
    for chunk in chunks:
        if chunk['title'] == title:
            return chunk
    return None


def search_chunks_by_keyword(chunks: List[Dict], keyword: str, case_sensitive: bool = False) -> List[Dict]:
    """
    æ ¹æ®å…³é”®è¯æœç´¢æ–‡æ¡£å—
    
    Args:
        chunks (List[Dict]): æ–‡æ¡£å—åˆ—è¡¨
        keyword (str): æœç´¢å…³é”®è¯
        case_sensitive (bool): æ˜¯å¦åŒºåˆ†å¤§å°å†™
        
    Returns:
        List[Dict]: åŒ…å«å…³é”®è¯çš„æ–‡æ¡£å—åˆ—è¡¨
    """
    if not case_sensitive:
        keyword = keyword.lower()
    
    matching_chunks = []
    
    for chunk in chunks:
        search_text = chunk['content'] if case_sensitive else chunk['content'].lower()
        title_text = chunk['title'] if case_sensitive else chunk['title'].lower()
        
        if keyword in search_text or keyword in title_text:
            matching_chunks.append(chunk)
    
    return matching_chunks


def filter_chunks_by_size(chunks: List[Dict], min_chars: int = 0, max_chars: int = float('inf')) -> List[Dict]:
    """
    æ ¹æ®å¤§å°è¿‡æ»¤æ–‡æ¡£å—
    
    Args:
        chunks (List[Dict]): æ–‡æ¡£å—åˆ—è¡¨
        min_chars (int): æœ€å°å­—ç¬¦æ•°
        max_chars (int): æœ€å¤§å­—ç¬¦æ•°
        
    Returns:
        List[Dict]: è¿‡æ»¤åçš„æ–‡æ¡£å—åˆ—è¡¨
    """
    return [chunk for chunk in chunks if min_chars <= chunk['char_count'] <= max_chars]


def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºæ–‡æ¡£åˆ†å‰²åŠŸèƒ½
    """
    print("ğŸ”§ è…¾è®¯äº‘IMæ–‡æ¡£åˆ†å‰²å·¥å…·")
    print("=" * 60)
    
    # è¯»å–å’Œåˆ†å‰²æ–‡æ¡£
    file_path = "/Users/zhangtianqi/git_root/agent_homework/ch8/è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿå®Œæ•´æ–‡æ¡£.md"
    chunks = read_and_split_document(file_path, "###")
    
    if not chunks:
        print("âŒ æ–‡æ¡£åˆ†å‰²å¤±è´¥")
        return
    
    # æ‰“å°æ‘˜è¦
    print_chunks_summary(chunks)
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼
    metadata = {
        'source_file': file_path,
        'description': 'è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿæ–‡æ¡£ï¼ŒæŒ‰ä¸‰çº§æ ‡é¢˜åˆ†å‰²'
    }
    json_file = "è…¾è®¯äº‘IMæ–‡æ¡£å—_å¤„ç†å.json"
    save_chunks_to_json(chunks, json_file, metadata)
    
    # æ¼”ç¤ºæœç´¢åŠŸèƒ½
    print(f"\nğŸ” æœç´¢æ¼”ç¤ºï¼š")
    print("-" * 30)
    
    # æœç´¢åŒ…å«"æƒé™"çš„å—
    permission_chunks = search_chunks_by_keyword(chunks, "æƒé™")
    print(f"åŒ…å«'æƒé™'çš„æ–‡æ¡£å—: {len(permission_chunks)} ä¸ª")
    for chunk in permission_chunks:
        print(f"  - {chunk['title']}")
    
    # æŒ‰å¤§å°è¿‡æ»¤
    large_chunks = filter_chunks_by_size(chunks, min_chars=1000)
    print(f"\nğŸ“ å¤§äº1000å­—ç¬¦çš„æ–‡æ¡£å—: {len(large_chunks)} ä¸ª")
    for chunk in large_chunks:
        print(f"  - {chunk['title']} ({chunk['char_count']} å­—ç¬¦)")
    
    # æ˜¾ç¤ºä¸€ä¸ªå…·ä½“å—çš„å†…å®¹é¢„è§ˆ
    if chunks:
        print(f"\nğŸ“– ç¤ºä¾‹æ–‡æ¡£å—å†…å®¹é¢„è§ˆ:")
        print("-" * 40)
        sample_chunk = chunks[0]
        print(f"æ ‡é¢˜: {sample_chunk['title']}")
        print(f"å­—ç¬¦æ•°: {sample_chunk['char_count']}")
        print(f"å†…å®¹é¢„è§ˆ:")
        preview = sample_chunk['content'][:300] + "..." if len(sample_chunk['content']) > 300 else sample_chunk['content']
        print(preview)
    
    return chunks


if __name__ == "__main__":
    main()