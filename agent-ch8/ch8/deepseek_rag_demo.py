#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è…¾è®¯äº‘IMæ–‡æ¡£RAGç³»ç»Ÿ - é›†æˆDeepSeek LLM
å®ç°çœŸæ­£çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œä½¿ç”¨DeepSeekä½œä¸ºç”Ÿæˆæ¨¡å‹
"""

import json
import time
import os
from typing import List, Dict, Optional
from datetime import datetime

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from document_splitter import read_and_split_document
from real_vector_store import TencentIMVectorDatabase
from tencent_embeddings import TencentLKEEmbeddings

# å¯¼å…¥OpenAIåº“ï¼ˆç”¨äºDeepSeek APIï¼‰
try:
    from openai import OpenAI
except ImportError:
    print("âŒ è¯·å®‰è£…openaiåº“: pip install openai>=1.0.0")
    exit(1)


class DeepSeekRAGSystem:
    """åŸºäºDeepSeek LLMçš„è…¾è®¯äº‘IMæ–‡æ¡£RAGç³»ç»Ÿ"""
    
    def __init__(self, config_file: str = "vector_store_config.json"):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            config_file (str): é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_file)
        self.db_manager = None
        self.embeddings = None
        self.llm_client = None
        self._initialize_components()
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            return config
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            return {
                "tencent_cloud": {
                    "secret_id": "your_secret_id_here",
                    "secret_key": "your_secret_key_here",
                    "region": "ap-guangzhou"
                },
                "vector_database": {
                    "name": "tencent.im.db",
                    "batch_size": 3
                },
                "documents": {
                    "source_file": "./è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿå®Œæ•´æ–‡æ¡£.md",
                    "split_level": "###"
                }
            }
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        print("ğŸ”§ åˆå§‹åŒ–DeepSeek RAGç³»ç»Ÿç»„ä»¶...")
        
        # 1. è®¾ç½®DeepSeek API
        os.environ["OPENAI_API_KEY"] = "sk-97aa33f5e09f41bbb7a29def74ab334a"
        os.environ["OPENAI_BASE_URL"] = "https://api.deepseek.com"
        
        self.llm_client = OpenAI(
            api_key=os.environ["OPENAI_API_KEY"],
            base_url=os.environ["OPENAI_BASE_URL"]
        )
        self.model_name = "deepseek-chat"
        
        print("âœ… DeepSeek LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        
        # 2. æ£€æŸ¥APIå¯†é’¥
        secret_id = self.config['tencent_cloud']['secret_id']
        secret_key = self.config['tencent_cloud']['secret_key']
        
        if secret_id == "your_secret_id_here":
            print("âš ï¸ æ£€æµ‹åˆ°æ¨¡æ¿é…ç½®ï¼Œä½¿ç”¨å†…ç½®APIå¯†é’¥")
            secret_id = ""
            secret_key = ""
        
        # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = TencentLKEEmbeddings(
            secret_id=secret_id,
            secret_key=secret_key
        )
        
        # 4. åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = TencentIMVectorDatabase(
            secret_id=secret_id,
            secret_key=secret_key,
            db_name=self.config['vector_database']['name']
        )
        
        print("âœ… å‘é‡æ•°æ®åº“ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def setup_pipeline(self) -> bool:
        """
        è®¾ç½®å®Œæ•´çš„RAGæµæ°´çº¿
        
        Returns:
            bool: è®¾ç½®æ˜¯å¦æˆåŠŸ
        """
        print("ğŸš€ å¼€å§‹è®¾ç½®DeepSeek RAGæµæ°´çº¿")
        print("=" * 60)
        
        try:
            # 1. æµ‹è¯•DeepSeek APIè¿æ¥
            print("ğŸ§ª æµ‹è¯•DeepSeek APIè¿æ¥...")
            test_response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            print(f"âœ… DeepSeek APIè¿æ¥æˆåŠŸ: {test_response.choices[0].message.content}")
            
            # 2. æ£€æŸ¥æºæ–‡æ¡£
            source_file = self.config['documents']['source_file']
            if not os.path.exists(source_file):
                print(f"âŒ æºæ–‡æ¡£ä¸å­˜åœ¨: {source_file}")
                return False
            
            print(f"ğŸ“„ æ‰¾åˆ°æºæ–‡æ¡£: {source_file}")
            
            # 3. æ£€æŸ¥å‘é‡æ•°æ®åº“
            if not self.db_manager.load_existing_database():
                print("ğŸ“ å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º...")
                
                # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
                success = self.db_manager.create_database_from_markdown(
                    source_file,
                    batch_size=self.config['vector_database']['batch_size']
                )
                
                if not success:
                    print("âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥")
                    return False
            else:
                print("âœ… å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸ")
            
            # 4. æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
            db_info = self.db_manager.get_database_info()
            print(f"\nğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
            for key, value in db_info.items():
                print(f"   {key}: {value}")
            
            print("\nâœ… DeepSeek RAGæµæ°´çº¿è®¾ç½®å®Œæˆ!")
            return True
            
        except Exception as e:
            print(f"âŒ è®¾ç½®æµæ°´çº¿å¤±è´¥: {e}")
            return False
    
    def retrieve_documents(self, query: str, k: int = 5) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query (str): æŸ¥è¯¢æ–‡æœ¬
            k (int): è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            List[Dict]: æ£€ç´¢ç»“æœ
        """
        if not self.db_manager:
            print("âŒ æ•°æ®åº“ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return []
        
        print(f"ğŸ” æ£€ç´¢æŸ¥è¯¢: '{query}'")
        
        try:
            start_time = time.time()
            results = self.db_manager.search_documents(query, k=k)
            search_time = time.time() - start_time
            
            print(f"âœ… æ£€ç´¢å®Œæˆï¼Œç”¨æ—¶ {search_time:.3f}ç§’ï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
            for i, result in enumerate(results):
                print(f"   {i+1}. ğŸ“„ {result['title']} (ç›¸ä¼¼åº¦: {result['similarity_score']:.4f})")
            
            return results
            
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def build_context(self, results: List[Dict], max_tokens: int = 3000) -> str:
        """
        æ„å»ºRAGä¸Šä¸‹æ–‡
        
        Args:
            results (List[Dict]): æ£€ç´¢ç»“æœ
            max_tokens (int): æœ€å¤§tokenæ•°ï¼ˆä¼°ç®—ï¼‰
            
        Returns:
            str: æ„å»ºçš„ä¸Šä¸‹æ–‡
        """
        if not results:
            return ""
        
        context_parts = []
        current_length = 0
        
        for result in results:
            title = result['title']
            content = result['full_content']
            
            # æ ¼å¼åŒ–æ–‡æ¡£å—
            section = f"ã€{title}ã€‘\n{content}\n\n"
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶ï¼ˆç²—ç•¥ä¼°ç®—ï¼Œ1ä¸ªä¸­æ–‡å­—ç¬¦â‰ˆ1.5ä¸ªtokenï¼‰
            section_tokens = len(section) * 1.5
            if current_length + section_tokens <= max_tokens:
                context_parts.append(section)
                current_length += section_tokens
            else:
                # å¦‚æœè¶…å‡ºé™åˆ¶ï¼Œæˆªæ–­æœ€åä¸€ä¸ªæ–‡æ¡£
                remaining_tokens = max_tokens - current_length
                if remaining_tokens > 200:  # è‡³å°‘ä¿ç•™200ä¸ªtoken
                    remaining_chars = int(remaining_tokens / 1.5)
                    truncated = section[:remaining_chars] + "...\n\n"
                    context_parts.append(truncated)
                break
        
        context = "".join(context_parts)
        estimated_tokens = len(context) * 1.5
        print(f"ğŸ“ æ„å»ºä¸Šä¸‹æ–‡å®Œæˆï¼Œé•¿åº¦: {len(context)} å­—ç¬¦ï¼Œé¢„ä¼°: {estimated_tokens:.0f} tokens")
        
        return context
    
    def generate_answer_with_deepseek(self, query: str, context: str) -> Dict:
        """
        ä½¿ç”¨DeepSeek LLMç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query (str): ç”¨æˆ·æŸ¥è¯¢
            context (str): æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            Dict: ç”Ÿæˆç»“æœ
        """
        print("ğŸ¤– ä½¿ç”¨DeepSeekç”Ÿæˆç­”æ¡ˆ...")
        
        # æ„å»ºä¸“ä¸šçš„RAGæç¤ºè¯
#         system_prompt = """ä½ æ˜¯è…¾è®¯äº‘å³æ—¶é€šä¿¡IMçš„ä¸“ä¸šæŠ€æœ¯åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

# å›ç­”è¦æ±‚ï¼š
# 1. åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
# 2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
# 3. å›ç­”è¦ç»“æ„åŒ–ã€ä¸“ä¸šã€å‡†ç¡®
# 4. ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰æ ¼å¼æé«˜å¯è¯»æ€§
# 5. é‡ç‚¹ä¿¡æ¯ç”¨**ç²—ä½“**æ ‡æ³¨

# æ–‡æ¡£å†…å®¹ï¼š
# {context}"""

#         user_prompt = f"""åŸºäºä¸Šè¿°è…¾è®¯äº‘IMæ–‡æ¡£å†…å®¹ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

# {query}

# è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚"""

        try:
            start_time = time.time()
            
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt.format(context=context)},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2000,
                temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
                stream=False
            )
            
            generation_time = time.time() - start_time
            
            answer = response.choices[0].message.content
            
            print(f"âœ… DeepSeekç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œç”¨æ—¶: {generation_time:.3f}ç§’")
            
            return {
                "answer": answer,
                "generation_time": generation_time,
                "model": self.model_name,
                "prompt_tokens": response.usage.prompt_tokens if hasattr(response, 'usage') else 0,
                "completion_tokens": response.usage.completion_tokens if hasattr(response, 'usage') else 0,
                "total_tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0
            }
            
        except Exception as e:
            print(f"âŒ DeepSeekç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "generation_time": 0,
                "model": self.model_name,
                "error": str(e)
            }
    
    def run_deepseek_rag(self, query: str) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„DeepSeek RAGæµç¨‹
        
        Args:
            query (str): ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            Dict: å®Œæ•´çš„RAGç»“æœ
        """
        print(f"\nğŸ¯ å¼€å§‹DeepSeek RAGæµç¨‹å¤„ç†")
        print(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢: '{query}'")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # 1. æ–‡æ¡£æ£€ç´¢
            print("ğŸ” æ­¥éª¤1: å‘é‡æ£€ç´¢")
            retrieved_docs = self.retrieve_documents(query, k=5)
            
            if not retrieved_docs:
                return {
                    'query': query,
                    'error': 'æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£',
                    'timestamp': datetime.now().isoformat()
                }
            
            # 2. ä¸Šä¸‹æ–‡æ„å»º
            print("\nğŸ“ æ­¥éª¤2: ä¸Šä¸‹æ–‡æ„å»º")
            context = self.build_context(retrieved_docs, max_tokens=3000)
            
            # 3. DeepSeekç­”æ¡ˆç”Ÿæˆ
            print("\nğŸ¤– æ­¥éª¤3: DeepSeekç­”æ¡ˆç”Ÿæˆ")
            generation_result = self.generate_answer_with_deepseek(query, context)
            
            total_time = time.time() - start_time
            
            # 4. ç»“æœæ±‡æ€»
            result = {
                'query': query,
                'answer': generation_result['answer'],
                'llm_info': {
                    'model': generation_result['model'],
                    'generation_time': generation_result['generation_time'],
                    'prompt_tokens': generation_result.get('prompt_tokens', 0),
                    'completion_tokens': generation_result.get('completion_tokens', 0),
                    'total_tokens': generation_result.get('total_tokens', 0)
                },
                'retrieved_documents': [
                    {
                        'title': doc['title'],
                        'similarity_score': doc['similarity_score'],
                        'content_preview': doc['content_preview']
                    }
                    for doc in retrieved_docs
                ],
                'context_length': len(context),
                'total_processing_time': round(total_time, 3),
                'timestamp': datetime.now().isoformat()
            }
            
            if 'error' in generation_result:
                result['llm_error'] = generation_result['error']
            
            print(f"\nâœ… DeepSeek RAGæµç¨‹å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.3f}ç§’")
            return result
            
        except Exception as e:
            print(f"âŒ DeepSeek RAGæµç¨‹å¤±è´¥: {e}")
            return {
                'query': query,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def run_batch_demo(self) -> Dict:
        """
        è¿è¡Œæ‰¹é‡DeepSeek RAGæ¼”ç¤ºæµ‹è¯•
        
        Returns:
            Dict: æ‰¹é‡æµ‹è¯•ç»“æœ
        """
        print("ğŸš€ å¼€å§‹æ‰¹é‡DeepSeek RAGæ¼”ç¤ºæµ‹è¯•")
        print("=" * 80)
        
        # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
        test_queries = [
            "ç›´æ’­ç¾¤AVChatRoomæœ‰ä»€ä¹ˆç‰¹ç‚¹å’Œé™åˆ¶ï¼Ÿ",
            "ç¤¾ç¾¤Communityæœ€å¤šæ”¯æŒå¤šå°‘äººï¼Ÿå¦‚ä½•ç”³è¯·æ›´å¤§è§„æ¨¡ï¼Ÿ",
            "å¦‚ä½•è®¾ç½®ç¾¤ç»„æƒé™ï¼Ÿæƒé™ç»„åŠŸèƒ½æ˜¯æ€æ ·çš„ï¼Ÿ",
            "Workç¾¤å’ŒPublicç¾¤åœ¨åŠŸèƒ½ä¸Šæœ‰ä»€ä¹ˆä¸»è¦åŒºåˆ«ï¼Ÿ",
            "ç¾¤æˆå‘˜èµ„æ–™åŒ…å«å“ªäº›å­—æ®µï¼Ÿè‡ªå®šä¹‰å­—æ®µæœ‰ä»€ä¹ˆç‰¹æ€§ï¼Ÿ",
            "ç¾¤ç»„çš„è‡ªåŠ¨å›æ”¶è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿä¸åŒç±»å‹ç¾¤ç»„æœ‰å·®å¼‚å—ï¼Ÿ"
        ]
        
        results = []
        total_start = time.time()
        
        for i, query in enumerate(test_queries):
            print(f"\n{'='*80}")
            print(f"æµ‹è¯• {i+1}/{len(test_queries)}")
            
            result = self.run_deepseek_rag(query)
            results.append(result)
            
            # ç®€è¦æ˜¾ç¤ºç»“æœ
            if 'error' not in result:
                print(f"\nğŸ“‹ DeepSeekç­”æ¡ˆé¢„è§ˆ:")
                preview = result['answer'][:200] + "..." if len(result['answer']) > 200 else result['answer']
                print(f"   {preview}")
                
                if 'llm_info' in result:
                    llm_info = result['llm_info']
                    print(f"\nğŸ’¬ LLMä½¿ç”¨ç»Ÿè®¡:")
                    print(f"   æ¨¡å‹: {llm_info['model']}")
                    print(f"   ç”Ÿæˆæ—¶é—´: {llm_info['generation_time']:.3f}ç§’")
                    print(f"   Tokens: {llm_info.get('total_tokens', 0)} (è¾“å…¥: {llm_info.get('prompt_tokens', 0)}, è¾“å‡º: {llm_info.get('completion_tokens', 0)})")
            
            # é¿å…APIé¢‘ç‡é™åˆ¶
            if i < len(test_queries) - 1:
                print("â±ï¸ ç­‰å¾…3ç§’...")
                time.sleep(3)
        
        total_time = time.time() - total_start
        
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“
        successful_results = [r for r in results if 'error' not in r]
        
        summary = {
            'total_queries': len(test_queries),
            'successful_queries': len(successful_results),
            'failed_queries': len(test_queries) - len(successful_results),
            'total_time': round(total_time, 3),
            'avg_time_per_query': round(total_time / len(test_queries), 3)
        }
        
        # è®¡ç®—LLMç»Ÿè®¡
        if successful_results:
            total_tokens = sum(r.get('llm_info', {}).get('total_tokens', 0) for r in successful_results)
            avg_generation_time = sum(r.get('llm_info', {}).get('generation_time', 0) for r in successful_results) / len(successful_results)
            
            summary.update({
                'total_tokens_used': total_tokens,
                'avg_generation_time': round(avg_generation_time, 3),
                'avg_tokens_per_query': round(total_tokens / len(successful_results), 1) if successful_results else 0
            })
        
        final_result = {
            'summary': summary,
            'results': results,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜ç»“æœ
        with open('deepseek_rag_demo_results.json', 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š DeepSeek RAGæ‰¹é‡æµ‹è¯•æ€»ç»“")
        print(f"{'='*80}")
        print(f"ğŸ“ æ€»æŸ¥è¯¢æ•°: {summary['total_queries']}")
        print(f"âœ… æˆåŠŸæŸ¥è¯¢: {summary['successful_queries']}")
        print(f"âŒ å¤±è´¥æŸ¥è¯¢: {summary['failed_queries']}")
        print(f"â±ï¸ æ€»è€—æ—¶: {summary['total_time']}ç§’")
        print(f"ğŸ“ˆ å¹³å‡è€—æ—¶: {summary['avg_time_per_query']}ç§’/æŸ¥è¯¢")
        
        if 'total_tokens_used' in summary:
            print(f"ğŸ’¬ æ€»Tokenä½¿ç”¨: {summary['total_tokens_used']}")
            print(f"ğŸ“Š å¹³å‡Token/æŸ¥è¯¢: {summary['avg_tokens_per_query']}")
            print(f"ğŸ¤– å¹³å‡ç”Ÿæˆæ—¶é—´: {summary['avg_generation_time']}ç§’")
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: deepseek_rag_demo_results.json")
        
        return final_result


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ è…¾è®¯äº‘IMæ–‡æ¡£DeepSeek RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 80)
    
    try:
        # 1. åˆå§‹åŒ–DeepSeek RAGç³»ç»Ÿ
        rag_system = DeepSeekRAGSystem()
        
        # 2. è®¾ç½®æµæ°´çº¿
        if not rag_system.setup_pipeline():
            print("âŒ DeepSeek RAGæµæ°´çº¿è®¾ç½®å¤±è´¥")
            return
        
        # 3. è¿è¡Œå•ä¸ªæŸ¥è¯¢æ¼”ç¤º
        print(f"\n{'='*80}")
        print("ğŸ” å•ä¸ªDeepSeek RAGæŸ¥è¯¢æ¼”ç¤º")
        print(f"{'='*80}")
        
        demo_query = "ç›´æ’­ç¾¤AVChatRoomæœ‰ä»€ä¹ˆç‰¹ç‚¹å’Œé™åˆ¶ï¼Ÿ"
        single_result = rag_system.run_deepseek_rag(demo_query)
        
        if 'error' not in single_result:
            print(f"\nğŸ“‹ DeepSeekå®Œæ•´ç­”æ¡ˆ:")
            print(f"{single_result['answer']}")
            print(f"\nğŸ“Š æ£€ç´¢åˆ° {len(single_result['retrieved_documents'])} ä¸ªç›¸å…³æ–‡æ¡£")
            print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {single_result['total_processing_time']}ç§’")
            if 'llm_info' in single_result:
                llm_info = single_result['llm_info']
                print(f"ğŸ¤– LLMä¿¡æ¯: {llm_info['model']}, ç”Ÿæˆæ—¶é—´: {llm_info['generation_time']:.3f}ç§’, Tokens: {llm_info.get('total_tokens', 0)}")
        
        # 4. è¿è¡Œæ‰¹é‡æ¼”ç¤º
        print(f"\n{'='*80}")
        print("ğŸ›ï¸ æ‰¹é‡DeepSeek RAGæŸ¥è¯¢æ¼”ç¤º")
        print(f"{'='*80}")
        
        batch_results = rag_system.run_batch_demo()
        
        print(f"\nğŸ‰ DeepSeek RAGæ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()
