#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£åˆ†å‰²å·¥å…·ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨document_splitteræ¨¡å—çš„å„ç§åŠŸèƒ½
"""

from document_splitter import (
    read_and_split_document,
    print_chunks_summary,
    save_chunks_to_json,
    get_chunk_by_title,
    search_chunks_by_keyword,
    filter_chunks_by_size
)


def basic_usage_example():
    """
    åŸºç¡€ä½¿ç”¨ç¤ºä¾‹ï¼šè¯»å–å’Œåˆ†å‰²æ–‡æ¡£
    """
    print("ğŸ”¥ åŸºç¡€ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. è¯»å–å’Œåˆ†å‰²æ–‡æ¡£
    chunks = read_and_split_document("è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿå®Œæ•´æ–‡æ¡£.md", "###")
    
    if chunks:
        # 2. æ˜¾ç¤ºæ‘˜è¦
        print_chunks_summary(chunks, show_details=False)
        
        # 3. ä¿å­˜ä¸ºJSON
        save_chunks_to_json(chunks, "example_output.json", {
            'description': 'ç¤ºä¾‹è¾“å‡º',
            'processor': 'example_usage.py'
        })
        
        return chunks
    
    return []


def search_examples(chunks):
    """
    æœç´¢åŠŸèƒ½ç¤ºä¾‹
    """
    print("\nğŸ” æœç´¢åŠŸèƒ½ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. æŒ‰å…³é”®è¯æœç´¢
    print("1. æœç´¢åŒ…å«'ç›´æ’­ç¾¤'çš„æ–‡æ¡£å—:")
    live_chunks = search_chunks_by_keyword(chunks, "ç›´æ’­ç¾¤")
    for chunk in live_chunks:
        print(f"   âœ… {chunk['title']} ({chunk['char_count']} å­—ç¬¦)")
    
    # 2. æŒ‰æ ‡é¢˜æŸ¥æ‰¾
    print("\n2. æŸ¥æ‰¾ç‰¹å®šæ ‡é¢˜çš„æ–‡æ¡£å—:")
    target_chunk = get_chunk_by_title(chunks, "æ¶ˆæ¯èƒ½åŠ›å·®å¼‚")
    if target_chunk:
        print(f"   âœ… æ‰¾åˆ°: {target_chunk['title']}")
        print(f"   ğŸ“„ å†…å®¹é•¿åº¦: {target_chunk['char_count']} å­—ç¬¦")
        print(f"   ğŸ“ ä½ç½®: ç¬¬{target_chunk['start_line']}-{target_chunk['end_line']}è¡Œ")
    else:
        print("   âŒ æœªæ‰¾åˆ°")
    
    # 3. å¤šå…³é”®è¯æœç´¢
    print("\n3. æœç´¢åŒ…å«'æƒé™'å’Œ'ç®¡ç†'çš„æ–‡æ¡£å—:")
    permission_chunks = search_chunks_by_keyword(chunks, "æƒé™")
    management_chunks = search_chunks_by_keyword(chunks, "ç®¡ç†")
    
    # æ‰¾åˆ°åŒæ—¶åŒ…å«ä¸¤ä¸ªå…³é”®è¯çš„å—
    common_chunks = []
    for chunk in permission_chunks:
        if chunk in management_chunks:
            common_chunks.append(chunk)
    
    print(f"   ğŸ“Š åŒ…å«'æƒé™': {len(permission_chunks)} ä¸ª")
    print(f"   ğŸ“Š åŒ…å«'ç®¡ç†': {len(management_chunks)} ä¸ª") 
    print(f"   ğŸ“Š åŒæ—¶åŒ…å«ä¸¤è€…: {len(common_chunks)} ä¸ª")
    
    for chunk in common_chunks:
        print(f"   âœ… {chunk['title']}")


def filtering_examples(chunks):
    """
    è¿‡æ»¤åŠŸèƒ½ç¤ºä¾‹
    """
    print("\nğŸ“ è¿‡æ»¤åŠŸèƒ½ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. æŒ‰å¤§å°è¿‡æ»¤
    print("1. æŒ‰æ–‡æ¡£å—å¤§å°è¿‡æ»¤:")
    
    small_chunks = filter_chunks_by_size(chunks, max_chars=500)
    medium_chunks = filter_chunks_by_size(chunks, min_chars=500, max_chars=1500)
    large_chunks = filter_chunks_by_size(chunks, min_chars=1500)
    
    print(f"   ğŸ“ å°å— (â‰¤500å­—ç¬¦): {len(small_chunks)} ä¸ª")
    print(f"   ğŸ“ ä¸­å— (500-1500å­—ç¬¦): {len(medium_chunks)} ä¸ª")
    print(f"   ğŸ“ å¤§å— (â‰¥1500å­—ç¬¦): {len(large_chunks)} ä¸ª")
    
    # 2. æ˜¾ç¤ºæ¯ç±»çš„å…·ä½“å—
    print("\n   å°å—åˆ—è¡¨:")
    for chunk in small_chunks:
        print(f"     â€¢ {chunk['title']} ({chunk['char_count']} å­—ç¬¦)")
    
    print("\n   å¤§å—åˆ—è¡¨:")
    for chunk in large_chunks:
        print(f"     â€¢ {chunk['title']} ({chunk['char_count']} å­—ç¬¦)")


def content_analysis_example(chunks):
    """
    å†…å®¹åˆ†æç¤ºä¾‹
    """
    print("\nğŸ“Š å†…å®¹åˆ†æç¤ºä¾‹")
    print("=" * 50)
    
    # 1. ç»Ÿè®¡å„ç±»ç¾¤ç»„ç±»å‹çš„æåŠæ¬¡æ•°
    group_types = ["Work", "Public", "Meeting", "AVChatRoom", "Community"]
    type_mentions = {}
    
    for group_type in group_types:
        mentions = 0
        for chunk in chunks:
            mentions += chunk['content'].count(group_type)
        type_mentions[group_type] = mentions
    
    print("1. ç¾¤ç»„ç±»å‹æåŠç»Ÿè®¡:")
    for group_type, count in sorted(type_mentions.items(), key=lambda x: x[1], reverse=True):
        print(f"   ğŸ“ˆ {group_type}: {count} æ¬¡")
    
    # 2. æ‰¾å‡ºæœ€é•¿å’Œæœ€çŸ­çš„æ–‡æ¡£å—
    longest_chunk = max(chunks, key=lambda x: x['char_count'])
    shortest_chunk = min(chunks, key=lambda x: x['char_count'])
    
    print(f"\n2. æ–‡æ¡£å—å¤§å°åˆ†æ:")
    print(f"   ğŸ“ æœ€é•¿: {longest_chunk['title']} ({longest_chunk['char_count']} å­—ç¬¦)")
    print(f"   ğŸ“ æœ€çŸ­: {shortest_chunk['title']} ({shortest_chunk['char_count']} å­—ç¬¦)")
    
    # 3. å†…å®¹å¯†åº¦åˆ†æï¼ˆè¡¨æ ¼å¯†é›†ç¨‹åº¦ï¼‰
    print(f"\n3. è¡¨æ ¼å¯†åº¦åˆ†æ:")
    for chunk in chunks:
        table_lines = chunk['content'].count('|')
        if table_lines > 10:  # åŒ…å«è¾ƒå¤šè¡¨æ ¼çš„å—
            density = table_lines / chunk['char_count'] * 1000  # æ¯1000å­—ç¬¦çš„è¡¨æ ¼æ ‡è®°æ•°
            print(f"   ğŸ“Š {chunk['title']}: {table_lines} ä¸ªè¡¨æ ¼æ ‡è®° (å¯†åº¦: {density:.1f}/1000å­—ç¬¦)")


def rag_preparation_example(chunks):
    """
    RAGç³»ç»Ÿå‡†å¤‡ç¤ºä¾‹
    """
    print("\nğŸ¤– RAGç³»ç»Ÿå‡†å¤‡ç¤ºä¾‹")
    print("=" * 50)
    
    # 1. ä¸ºæ¯ä¸ªå—ç”Ÿæˆå”¯ä¸€IDå’Œå‘é‡ç´¢å¼•å‡†å¤‡
    indexed_chunks = []
    for i, chunk in enumerate(chunks):
        indexed_chunk = chunk.copy()
        indexed_chunk['chunk_id'] = f"tencent_im_doc_{i:03d}"
        indexed_chunk['vector_ready'] = True
        indexed_chunk['embedding_text'] = f"{chunk['title']}\n{chunk['content']}"
        indexed_chunks.append(indexed_chunk)
    
    print(f"1. å·²ä¸º {len(indexed_chunks)} ä¸ªæ–‡æ¡£å—ç”Ÿæˆç´¢å¼•ID")
    
    # 2. æ ¹æ®å†…å®¹ç±»å‹åˆ†ç±»
    categories = {
        'table_heavy': [],  # è¡¨æ ¼å¯†é›†çš„å—
        'text_heavy': [],   # æ–‡æœ¬å¯†é›†çš„å—
        'mixed': []         # æ··åˆå†…å®¹çš„å—
    }
    
    for chunk in indexed_chunks:
        table_count = chunk['content'].count('|')
        text_lines = len([line for line in chunk['content'].split('\n') if line.strip() and not line.startswith('|')])
        
        if table_count > text_lines:
            categories['table_heavy'].append(chunk)
        elif text_lines > table_count * 2:
            categories['text_heavy'].append(chunk)
        else:
            categories['mixed'].append(chunk)
    
    print(f"\n2. å†…å®¹ç±»å‹åˆ†ç±»:")
    for category, chunks_list in categories.items():
        print(f"   ğŸ“‚ {category}: {len(chunks_list)} ä¸ªå—")
        for chunk in chunks_list[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
            print(f"      â€¢ {chunk['title']}")
    
    # 3. ä¿å­˜RAGå‡†å¤‡å°±ç»ªçš„æ•°æ®
    rag_data = {
        'metadata': {
            'total_chunks': len(indexed_chunks),
            'categories': {k: len(v) for k, v in categories.items()},
            'ready_for_embedding': True,
            'suggested_embedding_model': 'text-embedding-ada-002'
        },
        'chunks': indexed_chunks
    }
    
    save_chunks_to_json(indexed_chunks, "rag_ready_chunks.json", rag_data['metadata'])
    print(f"\nâœ… RAGå‡†å¤‡å°±ç»ªçš„æ•°æ®å·²ä¿å­˜åˆ°: rag_ready_chunks.json")


def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    """
    print("ğŸš€ è…¾è®¯äº‘IMæ–‡æ¡£åˆ†å‰²å·¥å…· - å®Œæ•´ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 80)
    
    # 1. åŸºç¡€ä½¿ç”¨
    chunks = basic_usage_example()
    
    if not chunks:
        print("âŒ æ— æ³•è¯»å–æ–‡æ¡£ï¼Œç¤ºä¾‹ç»ˆæ­¢")
        return
    
    # 2. æœç´¢ç¤ºä¾‹
    search_examples(chunks)
    
    # 3. è¿‡æ»¤ç¤ºä¾‹
    filtering_examples(chunks)
    
    # 4. å†…å®¹åˆ†æç¤ºä¾‹
    content_analysis_example(chunks)
    
    # 5. RAGå‡†å¤‡ç¤ºä¾‹
    rag_preparation_example(chunks)
    
    print(f"\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   â€¢ example_output.json - åŸºç¡€è¾“å‡º")
    print(f"   â€¢ rag_ready_chunks.json - RAGå‡†å¤‡å°±ç»ªçš„æ•°æ®")


if __name__ == "__main__":
    main()