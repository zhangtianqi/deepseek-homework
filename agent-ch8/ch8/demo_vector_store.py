#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡æ•°æ®åº“æ¼”ç¤ºè„šæœ¬
ä½¿ç”¨æ¨¡æ‹Ÿçš„embeddingæ¥æ¼”ç¤ºå‘é‡å­˜å‚¨å’Œæœç´¢åŠŸèƒ½
"""

import json
import os
import numpy as np
from typing import List, Dict
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
from document_splitter import read_and_split_document


class MockEmbeddings(Embeddings):
    """æ¨¡æ‹Ÿçš„åµŒå…¥æ¨¡å‹ï¼Œç”¨äºæ¼”ç¤ºç›®çš„"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
    
    def _get_embedding(self, text: str) -> List[float]:
        """æ ¹æ®æ–‡æœ¬å†…å®¹ç”Ÿæˆæ¨¡æ‹Ÿå‘é‡"""
        # ä½¿ç”¨æ–‡æœ¬çš„hashå€¼ä½œä¸ºç§å­ï¼Œç¡®ä¿ç›¸åŒæ–‡æœ¬ç”Ÿæˆç›¸åŒå‘é‡
        np.random.seed(hash(text) % (2**32))
        vector = np.random.normal(0, 1, self.dimension)
        # å½’ä¸€åŒ–å‘é‡
        vector = vector / np.linalg.norm(vector)
        return vector.tolist()
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æ¡£å‘é‡"""
        return [self._get_embedding(text) for text in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡"""
        return self._get_embedding(text)


def create_mock_vectorstore_demo():
    """
    ä½¿ç”¨æ¨¡æ‹Ÿembeddingåˆ›å»ºå‘é‡æ•°æ®åº“æ¼”ç¤º
    """
    print("ğŸ¯ è…¾è®¯äº‘IMæ–‡æ¡£å‘é‡æ•°æ®åº“æ¼”ç¤º (æ¨¡æ‹Ÿç‰ˆæœ¬)")
    print("=" * 60)
    
    # é…ç½®
    db_name = "tencent.im.db"
    persist_directory = f"./{db_name}"
    markdown_file = "/Users/zhangtianqi/git_root/agent_homework/ch8/è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿå®Œæ•´æ–‡æ¡£.md"
    
    try:
        # 1. è¯»å–å’Œåˆ†å‰²æ–‡æ¡£
        print("ğŸ“– è¯»å–å’Œåˆ†å‰²Markdownæ–‡æ¡£...")
        chunks = read_and_split_document(markdown_file, "###")
        
        if not chunks:
            print("âŒ æ— æ³•è¯»å–æˆ–åˆ†å‰²æ–‡æ¡£")
            return
        
        print(f"ğŸ“„ æˆåŠŸåˆ†å‰²å‡º {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        # 2. è½¬æ¢ä¸ºDocumentæ ¼å¼
        print("ğŸ“ è½¬æ¢ä¸ºLangchain Documentæ ¼å¼...")
        documents = []
        
        for i, chunk in enumerate(chunks):
            doc = Document(
                page_content=chunk['content'],
                metadata={
                    'chunk_id': f"tencent_im_{i:03d}",
                    'title': chunk['title'],
                    'level': chunk['level'],
                    'start_line': chunk['start_line'],
                    'end_line': chunk['end_line'],
                    'char_count': chunk['char_count'],
                    'word_count': chunk['word_count'],
                    'source': 'tencent_im_docs',
                    'chunk_index': i
                }
            )
            documents.append(doc)
        
        print(f"âœ… è½¬æ¢äº† {len(documents)} ä¸ªæ–‡æ¡£")
        
        # 3. åˆå§‹åŒ–æ¨¡æ‹ŸåµŒå…¥æ¨¡å‹
        print("ğŸ”§ åˆå§‹åŒ–æ¨¡æ‹ŸåµŒå…¥æ¨¡å‹...")
        embeddings = MockEmbeddings(dimension=384)
        
        # 4. åˆ›å»ºå‘é‡æ•°æ®åº“
        print("ğŸ’¾ åˆ›å»ºChromaå‘é‡æ•°æ®åº“...")
        
        # åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®åº“
        if os.path.exists(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
            print(f"ğŸ—‘ï¸ åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®åº“: {db_name}")
        
        # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_directory
        )
        
        print(f"âœ… å‘é‡æ•°æ®åº“ {db_name} åˆ›å»ºæˆåŠŸï¼")
        
        # 5. è·å–æ•°æ®åº“ä¿¡æ¯
        print("\nğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
        all_docs = vectorstore.get()
        print(f"   ğŸ“¦ æ•°æ®åº“åç§°: {db_name}")
        print(f"   ğŸ“ å­˜å‚¨è·¯å¾„: {persist_directory}")
        print(f"   ğŸ“„ æ–‡æ¡£æ€»æ•°: {len(all_docs['ids']) if all_docs['ids'] else 0}")
        print(f"   ğŸ¤– åµŒå…¥æ¨¡å‹: MockEmbeddings (384ç»´)")
        print(f"   ğŸ’¾ æ•°æ®åº“å¤§å°: {get_folder_size(persist_directory):.2f} MB")
        
        # 6. æ¼”ç¤ºæœç´¢åŠŸèƒ½
        print("\nğŸ” æœç´¢åŠŸèƒ½æ¼”ç¤º:")
        test_queries = [
            "ç›´æ’­ç¾¤æœ‰ä»€ä¹ˆç‰¹ç‚¹",
            "å¦‚ä½•è®¾ç½®ç¾¤ç»„æƒé™", 
            "ç¾¤æˆå‘˜ç®¡ç†åŠŸèƒ½",
            "ç¤¾ç¾¤Communityçš„ä¼˜åŠ¿",
            "æ¶ˆæ¯å­˜å‚¨å’Œæ¼«æ¸¸"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
            
            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            results = vectorstore.similarity_search_with_score(query, k=3)
            
            for i, (doc, score) in enumerate(results):
                title = doc.metadata.get('title', 'Unknown')
                char_count = doc.metadata.get('char_count', 0)
                content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                
                print(f"   {i+1}. ğŸ“„ {title}")
                print(f"      ğŸ“ {char_count} å­—ç¬¦ | ğŸ¯ ç›¸ä¼¼åº¦: {1-score:.4f}")
                print(f"      ğŸ“ å†…å®¹: {content_preview}")
        
        # 7. ä¿å­˜æœç´¢ç»“æœç¤ºä¾‹
        print(f"\nğŸ’¾ ä¿å­˜æ¼”ç¤ºç»“æœ...")
        demo_results = {
            'database_info': {
                'name': db_name,
                'total_documents': len(documents),
                'embedding_dimension': 384,
                'persist_directory': persist_directory
            },
            'search_examples': []
        }
        
        for query in test_queries[:2]:  # åªä¿å­˜å‰ä¸¤ä¸ªæŸ¥è¯¢çš„ç»“æœ
            results = vectorstore.similarity_search_with_score(query, k=2)
            query_results = {
                'query': query,
                'results': []
            }
            
            for doc, score in results:
                query_results['results'].append({
                    'title': doc.metadata.get('title'),
                    'similarity_score': float(1-score),
                    'char_count': doc.metadata.get('char_count'),
                    'content_preview': doc.page_content[:200]
                })
            
            demo_results['search_examples'].append(query_results)
        
        with open('vector_store_demo_results.json', 'w', encoding='utf-8') as f:
            json.dump(demo_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š æ¼”ç¤ºç»“æœå·²ä¿å­˜åˆ°: vector_store_demo_results.json")
        
        return vectorstore
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None


def get_folder_size(folder_path: str) -> float:
    """è·å–æ–‡ä»¶å¤¹å¤§å°ï¼ˆMBï¼‰"""
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(folder_path):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                total_size += os.path.getsize(file_path)
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    except Exception:
        return 0.0


def test_vector_operations(vectorstore):
    """
    æµ‹è¯•å‘é‡æ•°æ®åº“çš„å„ç§æ“ä½œ
    """
    if not vectorstore:
        print("âŒ å‘é‡æ•°æ®åº“æœªåˆ›å»ºï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    print("\nğŸ§ª å‘é‡æ•°æ®åº“æ“ä½œæµ‹è¯•:")
    print("=" * 40)
    
    # 1. æµ‹è¯•ä¸åŒç±»å‹çš„æŸ¥è¯¢
    query_tests = [
        ("åŠŸèƒ½æŸ¥è¯¢", "ç›´æ’­ç¾¤æ”¯æŒå“ªäº›åŠŸèƒ½"),
        ("å¯¹æ¯”æŸ¥è¯¢", "Workç¾¤å’ŒPublicç¾¤çš„åŒºåˆ«"),
        ("é…ç½®æŸ¥è¯¢", "å¦‚ä½•é…ç½®ç¾¤ç»„æƒé™"),
        ("é™åˆ¶æŸ¥è¯¢", "ç¾¤ç»„æœ‰å“ªäº›é™åˆ¶")
    ]
    
    for test_type, query in query_tests:
        print(f"\nğŸ“‹ {test_type}: '{query}'")
        results = vectorstore.similarity_search(query, k=2)
        
        for i, doc in enumerate(results):
            title = doc.metadata.get('title', 'Unknown')
            print(f"   {i+1}. {title}")
    
    # 2. æµ‹è¯•å…ƒæ•°æ®è¿‡æ»¤
    print(f"\nğŸ” å…ƒæ•°æ®è¿‡æ»¤æµ‹è¯•:")
    
    # æŸ¥æ‰¾åŒ…å«"æƒé™"çš„æ–‡æ¡£
    all_docs = vectorstore.get()
    permission_docs = []
    
    for i, doc_content in enumerate(all_docs['documents']):
        if 'æƒé™' in doc_content:
            metadata = all_docs['metadatas'][i]
            permission_docs.append(metadata.get('title', 'Unknown'))
    
    print(f"   åŒ…å«'æƒé™'çš„æ–‡æ¡£å—: {len(permission_docs)} ä¸ª")
    for title in permission_docs[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"   - {title}")


def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸš€ è…¾è®¯äº‘IMæ–‡æ¡£å‘é‡æ•°æ®åº“å®Œæ•´æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºå‘é‡æ•°æ®åº“æ¼”ç¤º
    vectorstore = create_mock_vectorstore_demo()
    
    # æµ‹è¯•å‘é‡æ“ä½œ
    test_vector_operations(vectorstore)
    
    print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"   â€¢ tencent.im.db/ - å‘é‡æ•°æ®åº“ç›®å½•")
    print(f"   â€¢ vector_store_demo_results.json - æ¼”ç¤ºç»“æœ")
    
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print(f"   1. æ›¿æ¢MockEmbeddingsä¸ºTencentLKEEmbeddings")
    print(f"   2. é…ç½®çœŸå®çš„è…¾è®¯äº‘APIå¯†é’¥")
    print(f"   3. é›†æˆåˆ°RAGç³»ç»Ÿä¸­")


if __name__ == "__main__":
    main()