#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è…¾è®¯äº‘æ–‡æ¡£URLä¿å­˜ä¸ºMarkdownçš„å·¥å…·
åŠŸèƒ½ï¼šä»æŒ‡å®šURLè·å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼ä¿å­˜
"""

import requests
from bs4 import BeautifulSoup
import html2text
import os
import re
from urllib.parse import urljoin, urlparse
from datetime import datetime


def fetch_url_content(url):
    """
    è·å–URLçš„ç½‘é¡µå†…å®¹
    
    Args:
        url (str): ç›®æ ‡URL
        
    Returns:
        tuple: (html_content, title, status_code)
    """
    try:
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        print(f"æ­£åœ¨è·å–URLå†…å®¹: {url}")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # ç¡®ä¿æ­£ç¡®çš„ç¼–ç 
        response.encoding = response.apparent_encoding or 'utf-8'
        
        # è§£æHTMLè·å–æ ‡é¢˜
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.find('title')
        title = title.text.strip() if title else "æ— æ ‡é¢˜"
        
        print(f"æˆåŠŸè·å–å†…å®¹ï¼Œæ ‡é¢˜: {title}")
        return response.text, title, response.status_code
        
    except requests.exceptions.RequestException as e:
        print(f"è·å–URLå†…å®¹å¤±è´¥: {e}")
        return None, None, None


def clean_html_content(html_content, base_url):
    """
    æ¸…ç†HTMLå†…å®¹ï¼Œæå–ä¸»è¦å†…å®¹
    
    Args:
        html_content (str): åŸå§‹HTMLå†…å®¹
        base_url (str): åŸºç¡€URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹é“¾æ¥
        
    Returns:
        str: æ¸…ç†åçš„HTMLå†…å®¹
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
    for tag in soup(['script', 'style', 'nav', 'footer', 'aside', 'header']):
        tag.decompose()
    
    # ç§»é™¤å¹¿å‘Šå’Œæ— å…³å†…å®¹çš„div
    for div in soup.find_all('div', class_=re.compile(r'(ad|advertisement|sidebar|menu|nav)', re.I)):
        div.decompose()
    
    # å¤„ç†ç›¸å¯¹URLè½¬ä¸ºç»å¯¹URL
    for link in soup.find_all('a', href=True):
        link['href'] = urljoin(base_url, link['href'])
    
    for img in soup.find_all('img', src=True):
        img['src'] = urljoin(base_url, img['src'])
    
    return str(soup)


def html_to_markdown(html_content, title=""):
    """
    å°†HTMLå†…å®¹è½¬æ¢ä¸ºMarkdownæ ¼å¼
    
    Args:
        html_content (str): HTMLå†…å®¹
        title (str): æ–‡æ¡£æ ‡é¢˜
        
    Returns:
        str: Markdownæ ¼å¼å†…å®¹
    """
    # é…ç½®html2textè½¬æ¢å™¨
    h = html2text.HTML2Text()
    h.ignore_links = False  # ä¿ç•™é“¾æ¥
    h.ignore_images = False  # ä¿ç•™å›¾ç‰‡
    h.ignore_emphasis = False  # ä¿ç•™å¼ºè°ƒæ ¼å¼
    h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
    h.unicode_snob = True  # å¤„ç†Unicodeå­—ç¬¦
    h.escape_snob = True  # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
    
    # è½¬æ¢ä¸ºMarkdown
    markdown_content = h.handle(html_content)
    
    # æ·»åŠ æ–‡æ¡£å¤´éƒ¨ä¿¡æ¯
    header = f"""# {title}

> æ¥æº: [è…¾è®¯äº‘æ–‡æ¡£](https://cloud.tencent.com/document/product/269/1502)
> è·å–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

"""
    
    return header + markdown_content


def clean_markdown(markdown_content):
    """
    æ¸…ç†Markdownå†…å®¹ï¼Œç§»é™¤å¤šä½™çš„ç©ºè¡Œå’Œæ ¼å¼é—®é¢˜
    
    Args:
        markdown_content (str): åŸå§‹Markdownå†…å®¹
        
    Returns:
        str: æ¸…ç†åçš„Markdownå†…å®¹
    """
    # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
    lines = markdown_content.split('\n')
    cleaned_lines = []
    prev_empty = False
    
    for line in lines:
        line = line.rstrip()  # ç§»é™¤è¡Œå°¾ç©ºæ ¼
        is_empty = len(line) == 0
        
        if is_empty and prev_empty:
            continue  # è·³è¿‡è¿ç»­çš„ç©ºè¡Œ
        
        cleaned_lines.append(line)
        prev_empty = is_empty
    
    return '\n'.join(cleaned_lines)


def save_to_file(content, filename):
    """
    ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
    
    Args:
        content (str): è¦ä¿å­˜çš„å†…å®¹
        filename (str): æ–‡ä»¶å
        
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"å†…å®¹å·²ä¿å­˜åˆ°: {filename}")
        return True
        
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False


def url_to_markdown(url, output_file=None):
    """
    å°†URLå†…å®¹è½¬æ¢ä¸ºMarkdownå¹¶ä¿å­˜
    
    Args:
        url (str): ç›®æ ‡URL
        output_file (str): è¾“å‡ºæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        
    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    # è·å–ç½‘é¡µå†…å®¹
    html_content, title, status_code = fetch_url_content(url)
    
    if html_content is None:
        print("æ— æ³•è·å–ç½‘é¡µå†…å®¹")
        return False
    
    print(f"HTTPçŠ¶æ€ç : {status_code}")
    
    # æ¸…ç†HTMLå†…å®¹
    cleaned_html = clean_html_content(html_content, url)
    
    # è½¬æ¢ä¸ºMarkdown
    markdown_content = html_to_markdown(cleaned_html, title)
    
    # æ¸…ç†Markdownæ ¼å¼
    markdown_content = clean_markdown(markdown_content)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if output_file is None:
        # ä»URLæˆ–æ ‡é¢˜ç”Ÿæˆæ–‡ä»¶å
        parsed_url = urlparse(url)
        safe_title = re.sub(r'[^\w\s-]', '', title).strip()[:50]  # é™åˆ¶é•¿åº¦å¹¶ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        if safe_title:
            output_file = f"{safe_title}.md"
        else:
            output_file = f"webpage_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    success = save_to_file(markdown_content, output_file)
    
    if success:
        print(f"è½¬æ¢å®Œæˆ! Markdownæ–‡ä»¶å¤§å°: {len(markdown_content.encode('utf-8'))} å­—èŠ‚")
    
    return success


def main():
    """
    ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°æˆ–ç›´æ¥è¿è¡Œ
    """
    # ç›®æ ‡URL
    target_url = "https://cloud.tencent.com/document/product/269/1502"
    
    print("=" * 60)
    print("è…¾è®¯äº‘æ–‡æ¡£è½¬Markdownå·¥å…·")
    print("=" * 60)
    
    # æ‰§è¡Œè½¬æ¢
    success = url_to_markdown(target_url, "è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿæ–‡æ¡£.md")
    
    if success:
        print("\nâœ… è½¬æ¢æˆåŠŸå®Œæˆ!")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥!")


def read_and_split_document(file_path, split_level="###"):
    """
    è¯»å–Markdownæ–‡æ¡£å¹¶æŒ‰æŒ‡å®šæ ‡é¢˜çº§åˆ«è¿›è¡Œåˆ†å‰²
    
    Args:
        file_path (str): æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        split_level (str): åˆ†å‰²çš„æ ‡é¢˜çº§åˆ«ï¼Œé»˜è®¤ä¸º"###"
        
    Returns:
        list: åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«æ ‡é¢˜å’Œå†…å®¹
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"æˆåŠŸè¯»å–æ–‡æ¡£: {file_path}")
        print(f"æ–‡æ¡£æ€»é•¿åº¦: {len(content)} å­—ç¬¦")
        
        # æŒ‰è¡Œåˆ†å‰²å†…å®¹
        lines = content.split('\n')
        
        # å­˜å‚¨åˆ†å‰²åçš„æ–‡æ¡£å—
        document_chunks = []
        current_chunk = {
            'title': '',
            'content': '',
            'level': 0,
            'start_line': 0,
            'end_line': 0
        }
        
        chunk_lines = []
        current_title = ""
        chunk_start_line = 0
        
        for i, line in enumerate(lines):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æŒ‡å®šçº§åˆ«çš„æ ‡é¢˜
            if line.startswith(split_level + " "):
                # å¦‚æœä¹‹å‰æœ‰ç§¯ç´¯çš„å†…å®¹ï¼Œä¿å­˜ä¸ºä¸€ä¸ªå—
                if chunk_lines and current_title:
                    chunk_content = '\n'.join(chunk_lines).strip()
                    if chunk_content:  # åªä¿å­˜éç©ºå†…å®¹
                        document_chunks.append({
                            'title': current_title,
                            'content': chunk_content,
                            'level': len(split_level),
                            'start_line': chunk_start_line + 1,
                            'end_line': i,
                            'word_count': len(chunk_content),
                            'char_count': len(chunk_content.encode('utf-8'))
                        })
                
                # å¼€å§‹æ–°çš„å—
                current_title = line.replace(split_level + " ", "").strip()
                chunk_lines = [line]  # åŒ…å«æ ‡é¢˜è¡Œ
                chunk_start_line = i
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                chunk_lines.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if chunk_lines and current_title:
            chunk_content = '\n'.join(chunk_lines).strip()
            if chunk_content:
                document_chunks.append({
                    'title': current_title,
                    'content': chunk_content,
                    'level': len(split_level),
                    'start_line': chunk_start_line + 1,
                    'end_line': len(lines),
                    'word_count': len(chunk_content),
                    'char_count': len(chunk_content.encode('utf-8'))
                })
        
        print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(document_chunks)} ä¸ªå—")
        return document_chunks
        
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        return []
    except Exception as e:
        print(f"è¯»å–æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []


def analyze_document_chunks(chunks):
    """
    åˆ†ææ–‡æ¡£å—çš„ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        chunks (list): æ–‡æ¡£å—åˆ—è¡¨
        
    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯
    """
    if not chunks:
        return {}
    
    total_chars = sum(chunk['char_count'] for chunk in chunks)
    total_words = sum(chunk['word_count'] for chunk in chunks)
    
    stats = {
        'total_chunks': len(chunks),
        'total_characters': total_chars,
        'total_words': total_words,
        'avg_chunk_size': total_chars // len(chunks) if chunks else 0,
        'min_chunk_size': min(chunk['char_count'] for chunk in chunks),
        'max_chunk_size': max(chunk['char_count'] for chunk in chunks),
        'chunks_info': []
    }
    
    for i, chunk in enumerate(chunks):
        stats['chunks_info'].append({
            'index': i,
            'title': chunk['title'],
            'char_count': chunk['char_count'],
            'lines': f"{chunk['start_line']}-{chunk['end_line']}"
        })
    
    return stats


def print_chunks_summary(chunks):
    """
    æ‰“å°æ–‡æ¡£å—çš„æ‘˜è¦ä¿¡æ¯
    
    Args:
        chunks (list): æ–‡æ¡£å—åˆ—è¡¨
    """
    if not chunks:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£å—")
        return
    
    print(f"\n{'='*60}")
    print(f"æ–‡æ¡£åˆ†å‰²æ‘˜è¦")
    print(f"{'='*60}")
    
    stats = analyze_document_chunks(chunks)
    
    print(f"æ€»å—æ•°: {stats['total_chunks']}")
    print(f"æ€»å­—ç¬¦æ•°: {stats['total_characters']:,}")
    print(f"å¹³å‡å—å¤§å°: {stats['avg_chunk_size']:,} å­—ç¬¦")
    print(f"æœ€å°å—å¤§å°: {stats['min_chunk_size']:,} å­—ç¬¦")
    print(f"æœ€å¤§å—å¤§å°: {stats['max_chunk_size']:,} å­—ç¬¦")
    
    print(f"\nå„å—è¯¦ç»†ä¿¡æ¯:")
    print(f"{'åºå·':<4} {'æ ‡é¢˜':<30} {'å­—ç¬¦æ•°':<8} {'è¡Œå·èŒƒå›´':<10}")
    print("-" * 60)
    
    for info in stats['chunks_info']:
        title = info['title'][:27] + "..." if len(info['title']) > 30 else info['title']
        print(f"{info['index']:<4} {title:<30} {info['char_count']:<8} {info['lines']:<10}")


def save_chunks_to_json(chunks, output_file):
    """
    å°†æ–‡æ¡£å—ä¿å­˜ä¸ºJSONæ ¼å¼
    
    Args:
        chunks (list): æ–‡æ¡£å—åˆ—è¡¨
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    try:
        import json
        
        # æ·»åŠ å…ƒæ•°æ®
        output_data = {
            'metadata': {
                'total_chunks': len(chunks),
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source_file': 'ch8/è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿå®Œæ•´æ–‡æ¡£.md'
            },
            'chunks': chunks
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"æ–‡æ¡£å—å·²ä¿å­˜åˆ°: {output_file}")
        return True
        
    except Exception as e:
        print(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        return False


def demo_document_splitting():
    """
    æ¼”ç¤ºæ–‡æ¡£åˆ†å‰²åŠŸèƒ½
    """
    print("=" * 60)
    print("è…¾è®¯äº‘IMæ–‡æ¡£åˆ†å‰²æ¼”ç¤º")
    print("=" * 60)
    
    # è¯»å–å’Œåˆ†å‰²æ–‡æ¡£
    file_path = "è…¾è®¯äº‘IMç¾¤ç»„ç³»ç»Ÿå®Œæ•´æ–‡æ¡£.md"
    chunks = read_and_split_document(file_path, "###")
    
    if not chunks:
        print("æ–‡æ¡£åˆ†å‰²å¤±è´¥")
        return
    
    # æ‰“å°æ‘˜è¦
    print_chunks_summary(chunks)
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªå—çš„å†…å®¹é¢„è§ˆ
    print(f"\n{'='*60}")
    print("å‰3ä¸ªæ–‡æ¡£å—å†…å®¹é¢„è§ˆ:")
    print(f"{'='*60}")
    
    for i, chunk in enumerate(chunks[:3]):
        print(f"\nğŸ“„ å— {i+1}: {chunk['title']}")
        print("-" * 40)
        # æ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
        preview = chunk['content'][:200] + "..." if len(chunk['content']) > 200 else chunk['content']
        print(preview)
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼
    json_file = "è…¾è®¯äº‘IMæ–‡æ¡£å—.json"
    save_chunks_to_json(chunks, json_file)
    
    return chunks


if __name__ == "__main__":
    # è¿è¡ŒåŸæœ‰çš„URLè½¬æ¢åŠŸèƒ½
    # main()
    
    # è¿è¡Œæ–‡æ¡£åˆ†å‰²æ¼”ç¤º
    demo_document_splitting()
